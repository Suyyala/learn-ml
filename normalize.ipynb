{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1D9eEQ24LuU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# applying an operation across dimention means reduce that dimention by applying\n",
        "# if i say mean across dim=0 -> dim 0 is across rows -> reduce to single row ->\n",
        "# for corresponding elements in the rows apply the operation\n",
        "\n",
        "# when say mean across the column, reduce to single column, meaning just take all\n",
        "# numbers across the column and find mean, which is nothing but mean across\n",
        "# the features\n",
        "x = torch.tensor([[1, 2, 3],\n",
        "                 [3, 4, 5]], dtype=torch.float32)\n",
        "print(x.shape)\n",
        "print(f'mean dim=0, {x.mean(dim=(0))}')\n",
        "print(f'mean dim=1, {x.mean(dim=(1))}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOf3oSivsDNJ",
        "outputId": "ea96d9a2-249e-4a24-a342-bd501635a2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3])\n",
            "mean dim=0, tensor([2., 3., 4.])\n",
            "mean dim=1, tensor([2., 4.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch normaliztion\n",
        "# IDEA : we scale features or inputs bewteen [-1, 1] or [0, 1] to put them in scale\n",
        "# However, with neural networks with layers where inputs get transformed\n",
        "# the scales may tip off as they go thru layers (explosive gradients or vanishing\n",
        "# gradients). To keep activation also in check on scale we can introduce normalizaiton\n",
        "# at layer level to stabilize the network, but we give network ability to learn\n",
        "# scale and shift so it can still use them when needed and turnoff where it doesn't\n",
        "# need\n",
        "\n",
        "# One thing to remember is you will need batch of data fed thru the network\n",
        "# to normalize with mean but some times we try to predict single image instead\n",
        "# of batch - which means may be we can't use the average since we have only one\n",
        "# I think thats we do use running average as new data comes into during inference\n",
        "\n",
        "\n",
        "# batch norm - is simply normalization at batch and spatial dimention level\n",
        "# meaning for an image, you compute mean over all pixels in the given batch for\n",
        "# each channel separately\n",
        "\n",
        "# This is useful for CNN kind of architectures as we have want to normalize\n",
        "# across all samples of a batch across spatial dimention (?)\n",
        "\n",
        "# mean across the batch of samples\n",
        "class BatchNormLayer(nn.Module):\n",
        "  def __init__(self, num_features):\n",
        "    super().__init__()\n",
        "    self.gamma = torch.nn.Parameter(torch.ones(num_features))\n",
        "    self.beta = torch.nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    # normalize input at batch dimention\n",
        "    x_mean = x.mean(dim=(0, 1, 2), keepdim=True)\n",
        "    x_var =  x.var(dim=(0, 1, 2), keepdim=True)\n",
        "    print(f'xmean {x_mean.shape}')\n",
        "    print(f'xvar {x_var.shape}')\n",
        "    x = (x - x_mean) / (x_var.sqrt() + 1e-5)\n",
        "    print(x.shape)\n",
        "    return self.gamma * x + self.beta\n",
        "\n",
        "# mean across the features\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, num_features=1):\n",
        "    super().__init__()\n",
        "    self.gamma = nn.Parameter(torch.ones(num_features))\n",
        "    self.beta = nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_mean = x.mean(dim=-1, keepdim=True)\n",
        "    x_var = x.var(dim=-1, keepdim=True)\n",
        "    x = (x - x_mean) / (x_var.sqrt() + 1e-5)\n",
        "    return self.gamma * x + self.beta\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_AGZtN-64Y9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normLayer = BatchNormLayer(3)\n",
        "x = torch.rand((32,  28, 28, 3))\n",
        "x_n = normLayer(x)\n",
        "print(x_n[0])\n",
        "\n",
        "y = torch.rand((32, 28), dtype=torch.float32)\n",
        "normLayer = LayerNorm(y.shape[1])\n",
        "x_n2 = normLayer(y)\n",
        "print(x_n2[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fodvrk5m9j1V",
        "outputId": "75a04056-4cc9-4131-d18c-ba3ec75b795a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xmean torch.Size([1, 1, 1, 3])\n",
            "xvar torch.Size([1, 1, 1, 3])\n",
            "torch.Size([32, 28, 28, 3])\n",
            "tensor([[[-1.6546, -1.1129,  1.6230],\n",
            "         [-0.7874, -1.6135, -0.3364],\n",
            "         [-1.6502,  0.2450, -0.5971],\n",
            "         ...,\n",
            "         [-1.2984,  0.1397,  1.0136],\n",
            "         [-0.1021, -0.1122,  1.5324],\n",
            "         [ 0.4109,  0.4648,  1.2886]],\n",
            "\n",
            "        [[ 1.5251, -0.8946, -1.6309],\n",
            "         [ 0.5354,  0.3831, -0.3087],\n",
            "         [-0.6289,  0.2811, -0.1496],\n",
            "         ...,\n",
            "         [ 0.8044, -1.0594, -0.3974],\n",
            "         [-1.1633, -0.2822,  0.7319],\n",
            "         [-0.1156,  1.1924,  0.5140]],\n",
            "\n",
            "        [[ 1.0720, -0.1923,  0.1814],\n",
            "         [-0.4184, -0.5187, -1.3650],\n",
            "         [ 0.1533, -0.7254, -1.1338],\n",
            "         ...,\n",
            "         [-1.1585,  0.4252,  1.4898],\n",
            "         [ 1.1304, -0.8287,  1.4644],\n",
            "         [-1.2427, -1.3963, -0.4700]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.5125,  0.8966,  1.5434],\n",
            "         [ 0.7702, -0.2155,  1.6845],\n",
            "         [ 0.8664,  0.5861,  0.6780],\n",
            "         ...,\n",
            "         [-0.7737,  1.6845,  0.1731],\n",
            "         [-0.6976,  0.2252,  1.3131],\n",
            "         [ 0.2647, -0.0387, -1.3099]],\n",
            "\n",
            "        [[-0.8235,  0.1489,  1.2549],\n",
            "         [ 1.4868, -0.1261, -0.3168],\n",
            "         [ 0.0963, -0.2161, -0.3624],\n",
            "         ...,\n",
            "         [ 0.9374,  0.8954, -1.3764],\n",
            "         [ 1.6247, -1.7243,  1.5727],\n",
            "         [-1.3972,  0.5563, -1.1037]],\n",
            "\n",
            "        [[-0.2707,  0.0933,  0.5119],\n",
            "         [-0.5594, -0.7636, -0.8371],\n",
            "         [ 0.5306, -0.1099, -0.0451],\n",
            "         ...,\n",
            "         [-0.7108,  0.5367,  0.7625],\n",
            "         [-0.3784,  0.8709, -0.9247],\n",
            "         [-0.7222, -0.7451,  1.3641]]], grad_fn=<SelectBackward0>)\n",
            "tensor([-0.4174, -0.7497, -1.0022,  1.4456, -0.8960,  0.5821,  1.6668, -0.7512,\n",
            "         0.8464, -0.1766, -1.0121,  1.5508, -1.7465,  1.0521, -1.0783, -1.4542,\n",
            "        -0.3000,  0.9812,  0.4020, -0.4715,  1.2159,  1.0558, -0.8186,  0.7478,\n",
            "        -1.1775, -0.2078,  0.1030,  0.6103], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    }
  ]
}